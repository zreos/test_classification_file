import os
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB  # 多项式贝叶斯分类器
from sklearn import metrics


# 对文档分词
def cut_file(path):
    '''
    :param path: 文档路径
    :return: 分词列表, 标签列表
    '''
    documents = []
    labels = []
    for root, dirs, files in os.walk(path):
        for file in files:
            label = root.split('\\')[-1]  # 读取文档标签: '女性' '体育' '文学' '校园'
            labels.append(label)
            filename = os.path.join(root, file)
            with open(filename, 'rb') as f:
                content = f.read()
                word_list = list(jieba.cut(content))
                documents.append(' '.join(word_list))  # 空格隔开分词
    return documents, labels


train_path = r"text_classification-master\text_classification\train"
test_path = r"text_classification-master\text_classification\test"
train_document, train_labels = cut_file(train_path)
test_document, test_labels = cut_file(test_path)

# 加载停用词表
stopwords = []
stopwords_path = r"E:\data_analys_work\text_classification-master\text_classification\stop\stopword.txt"
with open(stopwords_path, 'rb') as f:
    stopwords.append(line.strip() for line in f.readlines())
    # for line in f.readlines():
    # stopwords.append(line.encode('utf-8').decode('utf-8-sig').strip())  # encode().decode() 解决了诡异的 \ufeff 问题

# 计算单词的权重
train_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5)  # max_df 用来描述单词在文档中的最高出现率
train_features = train_tf.fit_transform(train_document)

# 生成朴素贝叶斯分类器
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)  # 分类器它 lei 了
# alpha 为平滑参数.
# 其值为 1 时使用 Laplace 平滑, 采用 +1 的方式, 来统计没有出现过的单词的概率. 样本数量很大的时候, +1 概率可以忽略不计.
# 其值 0 < alpha < 1 时, 使用Lidstone 平滑, alpha 值越小, 精度越高.

# 做预测
test_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5, vocabulary=train_tf.vocabulary_)
test_features = test_tf.fit_transform(test_document)
predicted_labels = clf.predict(test_features)

# 计算准确率
x = metrics.accuracy_score(test_labels, predicted_labels)
print(x)
